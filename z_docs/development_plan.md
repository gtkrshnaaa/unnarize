# Unnarize Development Roadmap

This document outlines the strategic development plan for the Unnarize JIT compiler, targeting world-class performance (1.2B+ ops/sec).

## ðŸŽ¯ Goal
Transform Unnarize into a **Full Native JIT-compiled language** with industry-leading performance.

**Performance Targets:**
- **Baseline**: 21M ops/sec (Tree-walker) - âœ… COMPLETED
- **Tier 1**: 800M ops/sec (Bytecode VM) - âœ… COMPLETED
- **Tier 2**: **1.2B ops/sec (Full Native JIT)** - ðŸš§ IN PROGRESS
- **Tier 3**: 2B+ ops/sec (Optimizing JIT) - FUTURE

---

## ðŸ“… Phases

### Phase 1: Bytecode Foundation âœ… COMPLETED
**Goal**: Replace AST tree-walking with a flat, cache-friendly bytecode VM.
- âœ… Design stack-based bytecode instruction set
- âœ… Implement AST-to-Bytecode compiler
- âœ… Build efficient direct-threaded interpreter loop
- âœ… Implement global/local variable scoping and stack management
- âœ… Optimize string handling (Interning)
- âœ… Verify stability with comprehensive benchmarks
- âœ… Clean compilation and installation system

**Result**: 800M ops/sec on compute-intensive code

---

### Phase 2: Full Native JIT Compilation ðŸš§ IN PROGRESS
**Goal**: **Replace bytecode interpreter with native x86-64 code generation for ALL hot paths.**

This is a **complete transformation** to Full Native JIT:
- All hot loops compiled to native code
- Automatic hotspot detection and compilation
- Seamless interpreter-to-JIT switching
- Complete opcode coverage in JIT compiler
- Advanced optimizations (register allocation, inline caching, constant folding)

**Target**: **1.2 billion operations/sec** (1.5x improvement over bytecode VM)

#### Infrastructure âœ… COMPLETED
- âœ… Executable memory manager (`mmap` RWX, W^X security)
- âœ… x86-64 machine code encoder (25+ instructions, manual encoding)
- âœ… Template-based JIT compiler core
- âœ… Function prolog/epilog generation

#### VM Integration ðŸš§ IN PROGRESS
- ðŸš§ Add JIT state to VM structure
- ðŸš§ Implement hotspot detection in interpreter
- ðŸš§ Automatic compilation trigger (threshold-based)
- ðŸš§ JIT cache management
- ðŸš§ Seamless interpreter-to-JIT switching

#### Complete Opcode Coverage ðŸ“‹ TODO
- ðŸ“‹ All arithmetic operations (ADD, SUB, MUL, DIV, MOD, NEG)
- ðŸ“‹ All comparison operations (LT, LE, GT, GE, EQ, NE)
- ðŸ“‹ Global variable access (LOAD/STORE/DEFINE)
- ðŸ“‹ Function calls (CALL, CALL_0/1/2, native calls)
- ðŸ“‹ Control flow (JUMP, JUMP_IF_FALSE/TRUE, LOOP)
- ðŸ“‹ Stack operations (POP, DUP)

#### Jump Patching ðŸ“‹ TODO
- ðŸ“‹ Two-pass compilation
- ðŸ“‹ Jump target tracking
- ðŸ“‹ Forward jump patching
- ðŸ“‹ Backward jump support (loops)

#### Optimizations for 1.2B ops/sec ðŸ“‹ TODO
- ðŸ“‹ Extended register allocation (8 registers: RAX-R9)
- ðŸ“‹ Inline caching for property access
- ðŸ“‹ Constant folding at compile time
- ðŸ“‹ Loop unrolling (4x for small loops)

#### Testing & Verification ðŸ“‹ TODO
- ðŸ“‹ Benchmark with ucoreTimer
- ðŸ“‹ Verify 1.2B ops/sec target on simple loops
- ðŸ“‹ Verify 1.0B ops/sec on arithmetic
- ðŸ“‹ Memory safety (valgrind clean)

---

### Phase 3: Optimizing JIT (Tier 3) - FUTURE
**Goal**: Apply advanced compiler optimizations using an Intermediate Representation (IR).

**Target**: 2B+ ops/sec

- **Profiler**:
    - Type profiling (polymorphism detection)
    - Advanced hotspot detection
- **Optimization Pipeline**:
    - SSA-based IR generation
    - Type specialization / monomorphization
    - Inlining of small functions
    - Loop unrolling and invariant code motion
    - Advanced register allocation (Graph coloring)
    - Dead code elimination

---

### Phase 4: Advanced Features - FUTURE
- **On-Stack Replacement (OSR)**: Switch to JIT code mid-loop
- **Deoptimization**: Bail out when speculative assumptions fail
- **SIMD**: Auto-vectorization for array operations
- **Garbage Collection**: Generational, incremental GC
- **Multi-tier JIT**: Baseline â†’ Optimizing tiers

---

## ðŸ“‚ Project Structure

```
src/
  bytecode/     - Bytecode compiler and interpreter (Tier 1)
  jit/          - Native Code Generation (Tier 2/3)
    memory.c    - Executable memory manager
    assembler.c - x86-64 instruction encoder
    jit_compiler.c - JIT compilation engine
  runtime/      - GC, Objects, Standard Library
  vm.c          - VM core and type definitions
```

---

## ðŸŽ¯ Current Focus: Phase 2 - Full Native JIT

**Objective**: Transform Unnarize to use native code generation for all hot execution paths.

**Key Principle**: This is NOT a hybrid approach. Hot code runs as **native x86-64 machine code**, not bytecode.

**Implementation Strategy**:
1. âœ… Build JIT infrastructure (memory, assembler, compiler core)
2. ðŸš§ Integrate JIT into VM (hotspot detection, automatic compilation)
3. ðŸ“‹ Complete opcode coverage (all operations in native code)
4. ðŸ“‹ Implement jump patching (proper control flow)
5. ðŸ“‹ Add optimizations (register allocation, inline caching)
6. ðŸ“‹ Benchmark and tune to hit 1.2B ops/sec target

**Success Criteria**:
- âœ… Simple loop: â‰¥ 1.2B ops/sec
- âœ… Arithmetic: â‰¥ 1.0B ops/sec
- âœ… Function calls: â‰¥ 500M ops/sec
- âœ… All examples work correctly
- âœ… No memory leaks

---

## ðŸ“Š Performance Evolution

| Phase | Execution Model | Performance | Status |
|-------|----------------|-------------|--------|
| Phase 0 | Tree-walking interpreter | 21M ops/sec | âœ… Done |
| Phase 1 | Bytecode VM | 800M ops/sec | âœ… Done |
| **Phase 2** | **Full Native JIT** | **1.2B ops/sec** | **ðŸš§ In Progress** |
| Phase 3 | Optimizing JIT | 2B+ ops/sec | ðŸ“‹ Future |

---

## ðŸš€ Next Steps

1. Complete VM integration (hotspot detection, JIT cache)
2. Implement jump patching (two-pass compilation)
3. Add remaining opcodes (DIV, MOD, globals, calls)
4. Optimize register allocation (8 registers)
5. Add inline caching for properties
6. Benchmark and tune to 1.2B ops/sec
7. Document and release Phase 2

**Estimated Time**: 13-18 hours of focused development
